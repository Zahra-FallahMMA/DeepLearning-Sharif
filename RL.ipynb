{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning Assignment  \n",
        "**Points:** 120\n",
        "\n",
        "---\n",
        "\n",
        "## **Deep Q-Network (DQN) on Cartpole v1**  \n",
        "**Homework 4**  \n",
        "**Deep Learning Course**  \n",
        "**Instructor:** Dr. Beigy  \n",
        "**Term:** Fall 2024  \n",
        "\n",
        "---\n",
        "\n",
        "### **Student Information**  \n",
        "- **Full Name:** _[Your Full Name]_  \n",
        "- **Student ID (SID):** _[Your SID]_  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vW4NPyeiHZdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Deep Q-Network (DQN) on CartPole v1  \n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we are going to implement **Deep Q-Network (DQN)** on the [CartPole v1 environment](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) using the **PyTorch** framework.  \n",
        "\n",
        "We will:  \n",
        "\n",
        "1. **Understand the Problem**:\n",
        "   - Explore the CartPole v1 environment provided by OpenAI Gym.\n",
        "   - Understand the state-space, action-space, and the reward structure of the environment.\n",
        "\n",
        "2. **Implement the DQN Algorithm**:\n",
        "   - Define the neural network architecture to approximate the Q-value function.\n",
        "   - Implement the experience replay buffer to store and sample transitions.\n",
        "   - Use the Îµ-greedy policy for balancing exploration and exploitation.\n",
        "   - Implement the target network to stabilize training.\n",
        "\n",
        "3. **Train the Agent**:\n",
        "   - Define the training loop where the agent interacts with the environment.\n",
        "   - Update the Q-network based on the Bellman equation.\n",
        "   - Periodically update the target network.\n",
        "\n",
        "4. **Evaluate Performance**:\n",
        "   - Track and visualize the agent's performance during training.\n",
        "   - Analyze the rewards and stability of the learned policy.\n",
        "\n",
        "---\n",
        "\n",
        "By the end of this notebook, you will gain a deeper understanding of how to implement DQN in practice and how to apply reinforcement learning techniques to solve control tasks.  \n"
      ],
      "metadata": {
        "id": "go90epKcHyiK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzXLC2Cw9vb4",
        "outputId": "2f7180ea-2408-463a-e2aa-b9b8b23f9363"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3aAsuZnF9vb2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Network Architecture (15 points)\n",
        "# Define the DQN network architecture\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # TODO: Define the network architecture\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass\n",
        "        pass"
      ],
      "metadata": {
        "id": "SpNoilqT9802"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQNAgent Initialization (15 points)\n",
        "# Define the DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Hyperparameters\n",
        "        # TODO: Adjust these hyperparameters if needed\n",
        "        self.gamma = 0.95    # discount factor\n",
        "        self.epsilon = 1.0   # initial exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.batch_size = 64\n",
        "\n",
        "        # TODO: Initialize the replay memory\n",
        "\n",
        "        # TODO: Create policy_net and target_net using DQN, and move them to self.device\n",
        "\n",
        "        # TODO: Copy policy_net weights to target_net\n",
        "\n",
        "        # TODO: Create optimizer (e.g., Adam) for policy_net parameters\n",
        "        pass\n",
        "\n",
        "    # Memory Handling (10 points)\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # TODO: Store the experience (state, action, reward, next_state, done) in the replay memory\n",
        "        pass\n",
        "\n",
        "    # Action Selection (10 points)\n",
        "    def act(self, state, evaluate=False):\n",
        "        # TODO: Implement epsilon-greedy action selection:\n",
        "        # If not in evaluate mode and random < epsilon, choose a random action\n",
        "        # Otherwise, choose the best action from the policy network\n",
        "        pass\n",
        "\n",
        "    # Experience Replay and Training Step (20 points)\n",
        "    def replay(self):\n",
        "        # TODO: Sample a minibatch from replay memory\n",
        "        # Compute target Q-values using target_net\n",
        "        # Compute predicted Q-values using policy_net\n",
        "        # Compute the loss and perform a gradient update step\n",
        "        # Decrease epsilon if above epsilon_min\n",
        "        pass\n",
        "\n",
        "    # Target Network Updates (5 points)\n",
        "    def update_target_network(self):\n",
        "        # TODO: Update target_net parameters with policy_net parameters\n",
        "        pass"
      ],
      "metadata": {
        "id": "ISeEDksZEw_R"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop (20 points)\n",
        "def train_cartpole():\n",
        "    # Create the environment\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # TODO: Create an instance of DQNAgent\n",
        "\n",
        "    episodes = 500\n",
        "    target_update_frequency = 10\n",
        "    scores = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        # TODO: Reset the environment\n",
        "\n",
        "        score = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # TODO: Use agent.act to get an action\n",
        "\n",
        "            # TODO: Take the action in the environment, observe next_state, reward, and done\n",
        "\n",
        "            # TODO: Store the experience in the agent's memory (agent.remember)\n",
        "\n",
        "            # TODO: Call agent.replay() to update the policy_net\n",
        "\n",
        "            # TODO: Update state and accumulate reward\n",
        "\n",
        "        # TODO: Update target network periodically\n",
        "\n",
        "        scores.append(score)\n",
        "        mean_score = np.mean(scores[-100:])\n",
        "\n",
        "        print(f\"Episode: {episode + 1}, Score: {score}, Average Score: {mean_score:.2f}, Epsilon: {0.0}\")  # TODO: Print agent.epsilon instead of 0.0\n",
        "\n",
        "        # TODO: Early stopping condition if solved\n",
        "\n",
        "    env.close()\n",
        "    # return agent, scores\n",
        "    pass"
      ],
      "metadata": {
        "id": "Ht65RRQv-BaC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization (5 points)\n",
        "def visualize_agent(agent, num_episodes=5):\n",
        "    \"\"\"\n",
        "    Visualize the trained agent in the environment.\n",
        "    \"\"\"\n",
        "    # TODO: Implement evaluation loop without exploration (evaluate=True).\n",
        "    # For each episode:\n",
        "    # 1. Reset environment\n",
        "    # 2. Render and step through with the agent's policy actions\n",
        "    # 3. Print the score at the end of each episode\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "qkB9R-ap_TQ3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training the agent...\")\n",
        "# TODO: Train the agent by calling train_cartpole and store the trained agent\n",
        "# agent, scores = train_cartpole()\n",
        "\n",
        "print(\"\\nStarting visualization...\")\n",
        "# TODO: Visualize the trained agent by calling visualize_agent(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc3RbBJW-Z9-",
        "outputId": "6f51f604-8773-4674-f69d-9f110a406af7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the agent...\n",
            "\n",
            "Starting visualization...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### â **Question**\n",
        "\n",
        "#### *(20 points)*  \n",
        "> **Why is it important to maintain both a policy network and a target network in Deep Q-Network training, and how does the use of a target network help stabilize learning?**  \n"
      ],
      "metadata": {
        "id": "a6qXVelELaLK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IrNtO1-kLgmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}