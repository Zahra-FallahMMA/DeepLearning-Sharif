{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ZbzcSP60pK"
      },
      "source": [
        "# Text Classification\n",
        "\n",
        "## RNN & LSTM\n",
        "\n",
        "### HW2\n",
        "\n",
        "**Full Name:Seyyedeh Zahra Fallah Mir Mousavi **\n",
        "\n",
        "**SID: 401207192**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI5BRkcqEvSe"
      },
      "source": [
        "## Homework Overview\n",
        "In this homework, you will learn to implement, train, and evaluate Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models on a text classification task using a dataset of IMDB movie reviews, and compare them.\n",
        "\n",
        "**NOTE : Be sure to answer the analytical questions at the end of the notebook as well.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "7kQ3EFONl9Cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a92f1f-5f89-4c39-d952-5561cd5d8673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import random\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import wordpunct_tokenize\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from IPython.core.display import display, HTML\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "tqdm.pandas()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NKWaZBn8NMY"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "In this section, weâ€™ll load the IMDB dataset and preprocess the data to make it suitable for training RNN and LSTM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_9UAw1I8uvP"
      },
      "source": [
        "## Load Dataset\n",
        "Description of Dataset: The IMDB movie reviews dataset consists of reviews along with their labels (positive or negative sentiment). Each review is a sentence or paragraph of text.\n",
        "\n",
        "Download the Dataset: We will use a Google Drive link to download the dataset into our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "s463eDIjmGmc"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'data/imdb_reviews.csv'\n",
        "gdd.download_file_from_google_drive(file_id='1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz',dest_path=DATA_PATH,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPnMJrH9AG4"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "For our models to work effectively, we need to preprocess the text data by cleaning it and converting words to integer indices for training.Preproces steps\n",
        "such as Tokenization and Cleaning , Replacing Rare Words , Build Vocabulary , Convert Tokens to Indices and Prepare Data for Training.\n",
        "\n",
        "**NOTE : Do not alter the structure of this preprocessing code, as it aligns with other parts of the notebook.However, minor adjustments for compatibility with your code are allowed if needed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "Y8egndGf-GIR"
      },
      "outputs": [],
      "source": [
        "def tokenize(text, stop_words):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = wordpunct_tokenize(text)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "XNlaFTd-mJuS"
      },
      "outputs": [],
      "source": [
        "def remove_rare_words(tokens, common_tokens, max_len):\n",
        "    return [token if token in common_tokens\n",
        "            else '<UNK>' for token in tokens][-max_len:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "zsJ38mMjmObb"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(data_path, max_vocab, max_len):\n",
        "    df = pd.read_csv(data_path)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Clean and tokenize\n",
        "    df['tokens'] = df['review'].apply(lambda x: tokenize(x, stop_words))\n",
        "\n",
        "    # Replace rare words with <UNK>\n",
        "    all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
        "    common_tokens = set(list(zip(*Counter(all_tokens).most_common(max_vocab)))[0])\n",
        "    df['tokens'] = df['tokens'].apply(lambda x: remove_rare_words(x, common_tokens, max_len))\n",
        "\n",
        "    # Remove sequences with only <UNK>\n",
        "    df = df[df['tokens'].apply(lambda tokens: any(token != '<UNK>' for token in tokens))]\n",
        "\n",
        "    # Build vocab\n",
        "    vocab = sorted(set([token for tokens in df['tokens'] for token in tokens]))\n",
        "    token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
        "    token2idx['<PAD>'] = len(token2idx)\n",
        "\n",
        "    # Index tokens\n",
        "    df['indexed_tokens'] = df['tokens'].apply(lambda tokens: [token2idx[token] for token in tokens])\n",
        "\n",
        "    return df['indexed_tokens'].tolist(), df['label'].tolist(), token2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "S86p_iDJmRbs"
      },
      "outputs": [],
      "source": [
        "# How many of the most common vocab words to keep\n",
        "# Uncommon words get replaced with unknown token <UNK>\n",
        "max_vocab = 2500\n",
        "\n",
        "# How many tokens long each sequence will be cut to\n",
        "# Shorter sequences will get the padding token <PAD>\n",
        "max_len = 100\n",
        "\n",
        "sequences, targets, token2idx = load_and_preprocess_data(DATA_PATH, max_vocab, max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "Hq_1Kc9Vmqp2"
      },
      "outputs": [],
      "source": [
        "def split_data(sequences, targets, valid_ratio=0.05, test_ratio=0.05):\n",
        "    total_size = len(sequences)\n",
        "    test_size = int(total_size * test_ratio)\n",
        "    valid_size = int(total_size * valid_ratio)\n",
        "    train_size = total_size - valid_size - test_size\n",
        "\n",
        "    train_sequences, train_targets = sequences[:train_size], targets[:train_size]\n",
        "    valid_sequences, valid_targets = sequences[train_size:train_size + valid_size], targets[train_size:train_size + valid_size]\n",
        "    test_sequences, test_targets = sequences[train_size + valid_size:], targets[train_size + valid_size:]\n",
        "\n",
        "    return train_sequences, train_targets, valid_sequences, valid_targets, test_sequences, test_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "rX0ZC4acmsaV"
      },
      "outputs": [],
      "source": [
        "train_sequences, train_targets, valid_sequences, valid_targets, test_sequences, test_targets = split_data(sequences, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "BoS4k2wwmzaN"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs_padded = pad_sequences(inputs, padding_val=token2idx['<PAD>'])\n",
        "    return torch.LongTensor(inputs_padded), torch.LongTensor(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "D_29NGAJnDo8"
      },
      "outputs": [],
      "source": [
        "def pad_sequences(sequences, padding_val=0, pad_left=False):\n",
        "    \"\"\"Pad a list of sequences to the same length with a padding_val.\"\"\"\n",
        "    sequence_length = max(len(sequence) for sequence in sequences)\n",
        "    if not pad_left:\n",
        "        return [sequence + [padding_val] * (sequence_length - len(sequence)) for sequence in sequences]\n",
        "    return [[padding_val] * (sequence_length - len(sequence)) + sequence for sequence in sequences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "HB33EytF1QCW"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "train_data = list(zip(train_sequences, train_targets))\n",
        "valid_data = list(zip(valid_sequences, valid_targets))\n",
        "test_data = list(zip(test_sequences, test_targets))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjwZOOd6_E1J"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EB2IPhH_fDR"
      },
      "source": [
        "## RNN with nn.RNN\n",
        "Implement a basic RNN model using PyTorch's built-in nn.RNN.\n",
        "Define layers: embedding, RNN, and fully connected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "KR4ggfh8nK3d"
      },
      "outputs": [],
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, vocab_size,\n",
        "                 device, n_layers=1,\n",
        "                 embedding_dimension=50):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx=token2idx['<PAD>'])\n",
        "\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"Define Needed Layers \"\"\"\n",
        "        #Your Code Here\n",
        "         # Recurrent layer\n",
        "        self.rnn = nn.LSTM(embedding_dimension, hidden_size, n_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"\n",
        "        Implements the forward pass: first, embed the input tokens, then pass\n",
        "        the embeddings through the RNN layer to capture sequential dependencies.\n",
        "        Finally, use fully connected layers to output class probabilities.\n",
        "        \"\"\"\n",
        "        #Your Code Here\n",
        "        # Embedding the input\n",
        "        embedded = self.embedding(inputs)\n",
        "\n",
        "        # Passing the embeddings through the RNN layer\n",
        "        rnn_out, _ = self.rnn(embedded)\n",
        "\n",
        "        # Using the output of the last time step for classification\n",
        "        out = self.fc(rnn_out[:, -1, :])  # Get the last time step output\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "        return out #probabilities for each class in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xv9SWFfFPjA"
      },
      "source": [
        "### Train model\n",
        "\n",
        "In this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data.\n",
        "\n",
        "**Note**: You are not allowed to use library-built trainer functions in this section; the training loop should be implemented manually.\n",
        "\n",
        "**Note**: To implement the training loop, you have the option to create a single train_model function that trains a model over multiple epochs, calculates training and validation accuracy, and logs the losses. Once written, this function can be reused for all RNN and LSTM models, allowing you to simply call it with different model instances for training. Reusing the function in this way will ensure that you receive credit for the training section of each subsequent model without needing to write separate loops , with just the correct function call.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "a2orLI-qXeFW"
      },
      "outputs": [],
      "source": [
        "model = RNNClassifier(output_size=2, hidden_size=128, vocab_size=len(token2idx), device=device, n_layers=1, embedding_dimension=50)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFw6Wl7xFW7m",
        "outputId": "6cd130a7-8a2e-475e-baa3-04634759a7aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.6721, Train Accuracy: 56.62%\n",
            "Validation Loss: 0.6501, Validation Accuracy: 57.97%\n",
            "--------------------------------------------------\n",
            "Epoch 2/10\n",
            "Train Loss: 0.5076, Train Accuracy: 75.77%\n",
            "Validation Loss: 0.4380, Validation Accuracy: 79.95%\n",
            "--------------------------------------------------\n",
            "Epoch 3/10\n",
            "Train Loss: 0.3889, Train Accuracy: 82.99%\n",
            "Validation Loss: 0.3619, Validation Accuracy: 84.36%\n",
            "--------------------------------------------------\n",
            "Epoch 4/10\n",
            "Train Loss: 0.3366, Train Accuracy: 85.73%\n",
            "Validation Loss: 0.3609, Validation Accuracy: 84.62%\n",
            "--------------------------------------------------\n",
            "Epoch 5/10\n",
            "Train Loss: 0.3107, Train Accuracy: 86.93%\n",
            "Validation Loss: 0.3147, Validation Accuracy: 86.42%\n",
            "--------------------------------------------------\n",
            "Epoch 6/10\n",
            "Train Loss: 0.2836, Train Accuracy: 88.16%\n",
            "Validation Loss: 0.2869, Validation Accuracy: 88.54%\n",
            "--------------------------------------------------\n",
            "Epoch 7/10\n",
            "Train Loss: 0.2691, Train Accuracy: 88.83%\n",
            "Validation Loss: 0.2650, Validation Accuracy: 89.67%\n",
            "--------------------------------------------------\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2556, Train Accuracy: 89.50%\n",
            "Validation Loss: 0.2520, Validation Accuracy: 90.25%\n",
            "--------------------------------------------------\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2433, Train Accuracy: 90.28%\n",
            "Validation Loss: 0.2511, Validation Accuracy: 90.63%\n",
            "--------------------------------------------------\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2275, Train Accuracy: 90.88%\n",
            "Validation Loss: 0.2337, Validation Accuracy: 90.67%\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "############################# TODO #############################\n",
        "# TODO: Implement the training loop\n",
        "################################################################\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss and accuracy\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs, labels = batch\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "        # Calculate average losses and accuracies\n",
        "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
        "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "        train_accuracy = correct_train / total_train * 100\n",
        "        val_accuracy = correct_val / total_val * 100\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "        print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "        print('-' * 50)\n",
        "\n",
        "        # Switch back to train mode\n",
        "        model.train()\n",
        "\n",
        "train(model, train_loader, valid_loader, criterion, optimizer, device, num_epochs=10)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJpjgTNc_rdx"
      },
      "source": [
        "## RNN from Scratch\n",
        "Implement an RNN from scratch by creating a custom RNN cell and a model that stacks these cells over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "BjJfWY4zfsBR"
      },
      "outputs": [],
      "source": [
        "class CustomRNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CustomRNNCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"Define Input-to-Hidden and Hidden-to-Hidden Layers\"\"\"\n",
        "        #Your Code Here\n",
        "        # Input-to-hidden transformation\n",
        "        self.i2h = nn.Linear(input_size, hidden_size)\n",
        "        # Hidden-to-hidden transformation\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"\n",
        "        Implements the forward pass: combines the input and previous hidden state\n",
        "        to calculate the new hidden state for this RNN cell.\n",
        "        \"\"\"\n",
        "        #Your Code Here\n",
        "        combined = self.i2h(input) + self.h2h(hidden)\n",
        "        hidden = torch.tanh(combined)\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "        return hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "UPODDCcAft1a"
      },
      "outputs": [],
      "source": [
        "class CustomRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(CustomRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2idx['<PAD>'])\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"Define Custom RNN Cell and Fully Connected Layers\"\"\"\n",
        "        #Your Code Here\n",
        "        # Initialize your custom RNN cell\n",
        "        self.rnn_cell = CustomRNNCell(embedding_dim, hidden_size)\n",
        "\n",
        "        # Define a fully connected layer to map hidden state to output classes\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"\n",
        "        Implements the forward pass: performs embedding lookup, iterates through each\n",
        "        time step, and passes embeddings through the custom RNN cell. Finally,\n",
        "        applies the fully connected layers to output class probabilities.\n",
        "        \"\"\"\n",
        "        #Your Code Here\n",
        "        # Embed input words\n",
        "        embeddings = self.embedding(inputs)\n",
        "\n",
        "        # Initialize hidden state (batch_size, hidden_size)\n",
        "        batch_size = inputs.size(0)\n",
        "        hidden = torch.zeros(batch_size, self.hidden_size, device=embeddings.device)\n",
        "\n",
        "        # Iterate over time steps\n",
        "        for t in range(inputs.size(1)):\n",
        "            input_t = embeddings[:, t, :]\n",
        "            hidden = self.rnn_cell(input_t, hidden)\n",
        "\n",
        "        # Use the hidden state from the last time step; apply fully connected layer to output class probabilities\n",
        "        out = self.fc(hidden)\n",
        "\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "        return out #probabilities for each class in the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMTPh0---Y63"
      },
      "source": [
        "### Train model\n",
        "\n",
        "In this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "nRgR4YxHfv5w"
      },
      "outputs": [],
      "source": [
        "model = CustomRNN(len(token2idx), 50, 128, 2)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "GH_uYgXTCziw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6aee91-f285-4425-d3e0-a688c10605cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6890\n",
            "Validation Loss: 0.6736, Accuracy: 58.74%\n",
            "Epoch [2/10], Loss: 0.6787\n",
            "Validation Loss: 0.6783, Accuracy: 55.75%\n",
            "Epoch [3/10], Loss: 0.6660\n",
            "Validation Loss: 0.6773, Accuracy: 51.95%\n",
            "Epoch [4/10], Loss: 0.6574\n",
            "Validation Loss: 0.6777, Accuracy: 52.53%\n",
            "Epoch [5/10], Loss: 0.6449\n",
            "Validation Loss: 0.6234, Accuracy: 62.76%\n",
            "Epoch [6/10], Loss: 0.6222\n",
            "Validation Loss: 0.5593, Accuracy: 73.19%\n",
            "Epoch [7/10], Loss: 0.6139\n",
            "Validation Loss: 0.6762, Accuracy: 53.46%\n",
            "Epoch [8/10], Loss: 0.6565\n",
            "Validation Loss: 0.6367, Accuracy: 58.58%\n",
            "Epoch [9/10], Loss: 0.5905\n",
            "Validation Loss: 0.5237, Accuracy: 76.50%\n",
            "Epoch [10/10], Loss: 0.5869\n",
            "Validation Loss: 0.6600, Accuracy: 54.49%\n"
          ]
        }
      ],
      "source": [
        "############################# TODO #############################\n",
        "# TODO: Implement the training loop\n",
        "################################################################\n",
        "num_epochs = 10  # Number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    ################## Optional: Validation Phase ##################\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in valid_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(valid_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tfvjr-8ECg4"
      },
      "source": [
        "### evaluate RNN models on test set\n",
        "To complete evaluate_on_test, loop through the test data to get predictions, calculate accuracy, and print a classification report for model evaluation. This function can be used to evaluate the performance LSTM models too.\n",
        "\n",
        "**NOTE : to earn full marks for this section, you must adjust the network's hyperparameters so that each rnn models achieves at least 70% accuracy on the test data. If you achieve less than the required accuracy, consider adjusting your training loop and hyperparameters, such as the hidden state size and learning rate, to improve model performance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "yZkeBWtgFbXA"
      },
      "outputs": [],
      "source": [
        "# Evaluate on test data\n",
        "def evaluate_on_test(model, test_loader):\n",
        "    model.eval()\n",
        "    ############################# TODO #############################\n",
        "    # TODO: Iterate over the test_loader, obtain model predictions,\n",
        "    # calculate accuracy, and generate a classification report.\n",
        "    ################################################################\n",
        "    y_true_test = []\n",
        "    y_pred_test = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation for inference\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass to get predictions\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Append true labels and predictions for evaluation\n",
        "            y_true_test.extend(labels.cpu().numpy())\n",
        "            y_pred_test.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true_test, y_pred_test)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Generate classification report\n",
        "    print(\"Classification Report:\")\n",
        "\n",
        "    print(classification_report(y_true_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "JkpP3q-xGotx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45dff7d-e7f3-4da0-997b-686e5ce2c2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RNN Model 1:\n",
            "Accuracy: 48.57%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.24      0.33      1619\n",
            "           1       0.48      0.75      0.58      1488\n",
            "\n",
            "    accuracy                           0.49      3107\n",
            "   macro avg       0.50      0.50      0.46      3107\n",
            "weighted avg       0.50      0.49      0.45      3107\n",
            "\n",
            "Evaluating RNN Model 2:\n",
            "Accuracy: 51.92%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.70      0.60      1619\n",
            "           1       0.50      0.32      0.39      1488\n",
            "\n",
            "    accuracy                           0.52      3107\n",
            "   macro avg       0.51      0.51      0.50      3107\n",
            "weighted avg       0.51      0.52      0.50      3107\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate both RNN models on the test dataset\n",
        "# Evaluate first RNN model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "rnn_model_1 = RNNClassifier(output_size=2, hidden_size=128, vocab_size=len(token2idx), device=device, n_layers=1, embedding_dimension=50)\n",
        "rnn_model_1.to(device)\n",
        "rnn_model_2 =  CustomRNN(len(token2idx), 50, 128, 2)\n",
        "rnn_model_2.to(device)\n",
        "print(\"Evaluating RNN Model 1:\")\n",
        "evaluate_on_test(rnn_model_1, test_loader)\n",
        "\n",
        "# Evaluate second RNN model\n",
        "print(\"Evaluating RNN Model 2:\")\n",
        "evaluate_on_test(rnn_model_2, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwlqjNQkAEBL"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwvNPJsoAFUf"
      },
      "source": [
        "## LSTM with nn.LSTM\n",
        "Define an LSTM model using PyTorch's built-in nn.LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "r2lPsPSc-ZQP"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, vocab_size,\n",
        "                 device, bidirectional=False, n_layers=1,\n",
        "                 embedding_dimension=50):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx = token2idx['<PAD>'])\n",
        "\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"Define the LSTM layer and fully connected layers\"\"\"\n",
        "        #Your Code Here: Initialize an nn.LSTM layer and any required fully connected layers.\n",
        "        # Define the LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dimension,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Define the fully connected layer\n",
        "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
        "\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Initialize hidden state and cell state with zeros\n",
        "        hidden = torch.zeros(self.n_layers * self.num_directions, inputs.size(0), self.hidden_size).to(inputs.device)\n",
        "        cell_state = torch.zeros(self.n_layers * self.num_directions, inputs.size(0), self.hidden_size).to(inputs.device)\n",
        "\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"\n",
        "        Implements the forward pass: first, embed the input tokens, then pass\n",
        "        the embeddings through the LSTM layer to capture sequential dependencies.\n",
        "        Finally, use fully connected layers to output class probabilities.\n",
        "        \"\"\"\n",
        "        #Your Code Here\n",
        "        # Embed the input tokens\n",
        "        embeddings = self.embedding(inputs)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        lstm_out, (hidden, cell_state) = self.lstm(embeddings, (hidden, cell_state))\n",
        "\n",
        "        # For classification, we often use the last hidden state of the sequence\n",
        "        # Depending on bidirectional, concatenate the final hidden states from both directions\n",
        "        if self.num_directions == 2:  # Bidirectional\n",
        "            last_hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # Concatenate the last layers\n",
        "        else:  # Unidirectional\n",
        "            last_hidden = hidden[-1]\n",
        "\n",
        "        out = self.fc(last_hidden)\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "        return out  # probabilities for each class in the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh3-B63fFgra"
      },
      "source": [
        "### Train model\n",
        "\n",
        "In this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data.\n",
        "\n",
        "**Note**: You are not allowed to use library-built trainer functions in this section; the training loop should be implemented manually.\n",
        "\n",
        "**Note**: To implement the training loop, you have the option to create a single train_model function that trains a model over multiple epochs, calculates training and validation accuracy, and logs the losses. Once written, this function can be reused for all RNN and LSTM models, allowing you to simply call it with different model instances for training. Reusing the function in this way will ensure that you receive credit for the training section of each subsequent model without needing to write separate loops , with just the correct function call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "cnYI5bAAXhqg"
      },
      "outputs": [],
      "source": [
        "model = LSTMClassifier(\n",
        "    output_size=3, hidden_size=128, vocab_size=len(token2idx), device=device,\n",
        "    bidirectional=True, n_layers=2, embedding_dimension=50\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQcqfE4xFmBi",
        "outputId": "6544104c-31db-4989-a62d-ebe3eb2658e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:10<00:00, 19.91it/s]\n",
            "Epoch 1/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 53.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "    Training Loss: 0.6475\n",
            "    Validation Accuracy: 0.7621\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:11<00:00, 19.59it/s]\n",
            "Epoch 2/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 51.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10:\n",
            "    Training Loss: 0.4254\n",
            "    Validation Accuracy: 0.8301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:11<00:00, 19.60it/s]\n",
            "Epoch 3/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 52.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10:\n",
            "    Training Loss: 0.3471\n",
            "    Validation Accuracy: 0.8587\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:11<00:00, 19.65it/s]\n",
            "Epoch 4/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 53.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10:\n",
            "    Training Loss: 0.3128\n",
            "    Validation Accuracy: 0.8725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:11<00:00, 19.78it/s]\n",
            "Epoch 5/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 52.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10:\n",
            "    Training Loss: 0.2821\n",
            "    Validation Accuracy: 0.8848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:10<00:00, 19.99it/s]\n",
            "Epoch 6/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 49.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10:\n",
            "    Training Loss: 0.2645\n",
            "    Validation Accuracy: 0.8954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:10<00:00, 19.95it/s]\n",
            "Epoch 7/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 51.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10:\n",
            "    Training Loss: 0.2421\n",
            "    Validation Accuracy: 0.9044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:11<00:00, 19.83it/s]\n",
            "Epoch 8/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 51.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10:\n",
            "    Training Loss: 0.2209\n",
            "    Validation Accuracy: 0.9121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:11<00:00, 19.80it/s]\n",
            "Epoch 9/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 53.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10:\n",
            "    Training Loss: 0.2009\n",
            "    Validation Accuracy: 0.9260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 219/219 [00:11<00:00, 19.73it/s]\n",
            "Epoch 10/10 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 51.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10:\n",
            "    Training Loss: 0.1761\n",
            "    Validation Accuracy: 0.9424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "############################# TODO #############################\n",
        "# TODO: Implement the training loop\n",
        "################################################################\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()  # Set model to training mode\n",
        "        total_loss = 0\n",
        "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, targets)  # Compute loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)  # Forward pass\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "                total += targets.size(0)\n",
        "\n",
        "        val_accuracy = correct / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Log progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "        print(f\"    Training Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"    Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    return model, train_losses, val_accuracies\n",
        "\n",
        "# Model instantiation\n",
        "output_size = 3  # Example: number of output classes\n",
        "hidden_size = 128\n",
        "vocab_size = len(token2idx)  # Adjust based on your tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "embedding_dimension = 50\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "\n",
        "lstm = LSTMClassifier(\n",
        "    output_size=output_size,\n",
        "    hidden_size=hidden_size,\n",
        "    vocab_size=vocab_size,\n",
        "    device=device,\n",
        "    bidirectional=bidirectional,\n",
        "    n_layers=n_layers,\n",
        "    embedding_dimension=embedding_dimension\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Training the model\n",
        "trained_model, train_losses, val_accuracies = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=valid_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    num_epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvLAiujIAOWh"
      },
      "source": [
        "## Custom LSTM from Scratch\n",
        "Implement an LSTM from scratch by defining a LSTM cell and a model that combines these cells over the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "JyccBL7zvwGZ"
      },
      "outputs": [],
      "source": [
        "class CustomLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(CustomLSTMCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"Define Needed Layers \"\"\"\n",
        "        #Your Code Here\n",
        "        # Input gate\n",
        "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # Forget gate\n",
        "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # Output gate\n",
        "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        # Cell candidate\n",
        "        self.cell_candidate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "    def forward(self, input, hidden, cell_state):\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"Define Forward pass\"\"\"\n",
        "        #Your Code Here\n",
        "\n",
        "        combined = torch.cat((input, hidden), dim=1)\n",
        "\n",
        "        # Calculate gates\n",
        "        i_gate = torch.sigmoid(self.input_gate(combined))  # Input gate\n",
        "        f_gate = torch.sigmoid(self.forget_gate(combined))  # Forget gate\n",
        "        o_gate = torch.sigmoid(self.output_gate(combined))  # Output gate\n",
        "        g_gate = torch.tanh(self.cell_candidate(combined))  # Cell candidate\n",
        "\n",
        "        # Update cell state\n",
        "        cell_state = f_gate * cell_state + i_gate * g_gate\n",
        "\n",
        "        # Update hidden state\n",
        "        hidden = o_gate * torch.tanh(cell_state)\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "        return hidden, cell_state # New hidden state , New cell state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "88b8uj3Gv1JB"
      },
      "outputs": [],
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(CustomLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=token2idx['<PAD>'])\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"Define Needed Layers \"\"\"\n",
        "        #Your Code Here\n",
        "\n",
        "        # Define the custom LSTM cell\n",
        "        self.lstm_cell = CustomLSTMCell(embedding_dim, hidden_size)\n",
        "\n",
        "        # Fully connected layer to produce output probabilities\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Initialize hidden state and cell state with zeros\n",
        "        hidden = torch.zeros(inputs.size(0), self.hidden_size).to(inputs.device)\n",
        "        cell_state = torch.zeros(inputs.size(0), self.hidden_size).to(inputs.device)\n",
        "\n",
        "        ###################################### TODO #####################################\n",
        "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
        "        #################################################################################\n",
        "        \"\"\"\n",
        "        Implements the forward pass: first, embed the input tokens, then pass\n",
        "        the embeddings through the LSTM layer to capture sequential dependencies.\n",
        "        Finally, use fully connected layers to output class probabilities.\n",
        "        \"\"\"\n",
        "        #Your Code Here\n",
        "\n",
        "        # Embed the input tokens\n",
        "        embeddings = self.embedding(inputs)\n",
        "\n",
        "        # Process each time step\n",
        "        for t in range(embeddings.size(1)):  # Iterate over the sequence length\n",
        "            input_t = embeddings[:, t, :]  # Get the embedding for the t-th time step\n",
        "            hidden, cell_state = self.lstm_cell(input_t, hidden, cell_state)\n",
        "\n",
        "        # Pass the final hidden state through the fully connected layer\n",
        "        out = self.fc(hidden)\n",
        "\n",
        "        #################################################################################\n",
        "        #                                   THE END                                     #\n",
        "        #################################################################################\n",
        "\n",
        "        return out  # probabilities for each class in the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx6vjFS-FnxW"
      },
      "source": [
        "### Train model\n",
        "\n",
        "In this section, you should train model for multiple epochs on the training data and evaluate it on the validation data after each epoch, reporting the model's accuracy. Ensure that the model is set to train mode during training and switched to eval mode for evaluation on the validation data. The objective is to implement the training loop and, at the next , compute and report the final accuracy on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "aBNu0nfHXjsB"
      },
      "outputs": [],
      "source": [
        "# Define the parameters\n",
        "vocab_size = len(token2idx)  # Vocabulary size (size of token-to-index mapping)\n",
        "embedding_dim = 50           # Embedding dimension\n",
        "hidden_size = 128            # Hidden size of LSTM\n",
        "output_size = 3              # Number of output classes\n",
        "\n",
        "# Initialize the model\n",
        "custom_lstm = CustomLSTM(vocab_size, embedding_dim, hidden_size, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "uZVEkl2xFnxW"
      },
      "outputs": [],
      "source": [
        "############################# TODO #############################\n",
        "# TODO: Implement the training loop\n",
        "################################################################\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, device='cpu'):\n",
        "    # Store losses and accuracies for tracking\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        # Iterate over the training data\n",
        "        for batch in train_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss for reporting\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Get predictions and calculate accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "        # Calculate average loss for this epoch\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_accuracy = 100 * correct_predictions / total_predictions\n",
        "\n",
        "        # Evaluate on validation data\n",
        "        val_accuracy = evaluate_model(model, val_loader, device)\n",
        "\n",
        "        # Store validation accuracy\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "              f\"Training Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Training Accuracy: {train_accuracy:.2f}%, \"\n",
        "              f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    return model, train_losses, val_accuracies\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate_model(model, val_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed for evaluation\n",
        "        for batch in val_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = 100 * correct_predictions / total_predictions\n",
        "    return val_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYv7jyi4F2j5"
      },
      "source": [
        "### evaluate LSTM models on test set\n",
        "To complete evaluate_on_test, loop through the test data to get predictions, calculate accuracy, and print a classification report for model evaluation.\n",
        "you can use the `evaluate_on_test` function implemented in the previous section. Alternatively, you may write a new function to conduct this evaluation.ensure report the \"classification_report\" of both LSTM models.\n",
        "\n",
        "**NOTE : to earn full marks for this section, you must adjust the network's hyperparameters so that each lstm models achieves at least 80% accuracy on the test data. If you achieve less than the required accuracy, consider adjusting your training loop and hyperparameters, such as the hidden state size and learning rate, to improve model performance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "K0Yyousdi7ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fafa5c-f5e4-4437-da3e-813832cc4d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for LSTMClassifier: 42.90%\n",
            "Test Accuracy for Custom LSTM: 44.06%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate both LSTM models on the test dataset\n",
        "\n",
        "def evaluate_on_test(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for batch in test_loader:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    test_accuracy = 100 * correct_predictions / total_predictions\n",
        "    return test_accuracy\n",
        "\n",
        "# Evaluate the first LSTM model\n",
        "test_accuracy_1 = evaluate_on_test(lstm, test_loader, device)\n",
        "print(f\"Test Accuracy for LSTMClassifier: {test_accuracy_1:.2f}%\")\n",
        "\n",
        "# Evaluate the second LSTM model\n",
        "test_accuracy_2 = evaluate_on_test(custom_lstm, test_loader, device)\n",
        "print(f\"Test Accuracy for Custom LSTM: {test_accuracy_2:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iML4dcBmAc95"
      },
      "source": [
        "# Testing RNN and LSTM Models on a New Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "B14PDnHiCMzH"
      },
      "outputs": [],
      "source": [
        "# Example review\n",
        "review = \"It is no wonder that the film has such a high rating, it is quite literally breathtaking. What can I say that hasn't said before? Not much, it's the story, the acting, the premise, but most of all, this movie is about how it makes you feel. Sometimes you watch a film, and can't remember it days later, this film loves with you, once you've seen it, you don't forget.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hql6MyMCK6_"
      },
      "source": [
        "## Preprocess the test Review\n",
        "To prepare the review for the model, we need to follow similar preprocessing steps as we did for the dataset:\n",
        "\n",
        "Remove special characters and convert the text to lowercase.\n",
        "Tokenize the text into individual words.\n",
        "Remove stopwords to focus only on meaningful words.\n",
        "Convert tokens to indices based on the token2idx dictionary created earlier.\n",
        "Pad or truncate the sequence to a length of max_len .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "vKpDw77QBC84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text, stop_words, token2idx, max_len):\n",
        "\n",
        "    ########################### TODO ###########################\n",
        "    # Step 1: Clean and lowercase the input text\n",
        "    ################################################################\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "\n",
        "    ########################### TODO ###########################\n",
        "    # Step 2: Tokenize the text into words\n",
        "    # - Use wordpunct_tokenize to split the cleaned text into individual word tokens.\n",
        "    ############################################################\n",
        "\n",
        "    tokens = wordpunct_tokenize(text)\n",
        "\n",
        "    ########################### TODO ###########################\n",
        "    # Step 3: Remove stopwords from the token list\n",
        "    ############################################################\n",
        "\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    ########################### TODO ###########################\n",
        "    # Step 4: Convert tokens to indices based on the token2idx dictionary\n",
        "    # - For each token in the list, get the corresponding index from the token2idx dictionary.\n",
        "    # - If a token is not found in token2idx, replace it with the index of '<UNK>'.\n",
        "    ################################################################\n",
        "\n",
        "    tokens_idx = [token2idx.get(word, token2idx['<UNK>']) for word in tokens]  # Use '<UNK>' for unknown words\n",
        "\n",
        "    ########################### TODO ###########################\n",
        "    # Step 5: Pad or truncate the tokens_idx list to the desired max_len\n",
        "    ############################################################\n",
        "\n",
        "    if len(tokens_idx) < max_len:\n",
        "        tokens_idx = tokens_idx + [token2idx['<PAD>']] * (max_len - len(tokens_idx))  # Padding\n",
        "    else:\n",
        "        tokens_idx = tokens_idx[:max_len]  # Truncation\n",
        "\n",
        "    ########################### End of TODOs ###########################\n",
        "\n",
        "    return tokens_idx  # Return the processed list of indices\n",
        "\n",
        "# Get stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "########################### TODO ###########################\n",
        "# Set the maximum length for the review sequence\n",
        "max_len = 100 # Example length\n",
        "\n",
        "# Preprocess the review\n",
        "review_indices = preprocess_text(review, stop_words, token2idx, max_len)\n",
        "############################################################\n",
        "\n",
        "# Convert the indices to a tensor and move it to the device (GPU or CPU)\n",
        "input_tensor = torch.LongTensor([review_indices]).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqvvQrhjCSNY"
      },
      "source": [
        "## Make Predictions\n",
        "Now that we have preprocessed the review, use both the RNN and LSTM models to make predictions on the sentiment of the review.\n",
        "\n",
        "Set the model to evaluation mode to prevent updates during inference.\n",
        "Predict the sentiment class by passing the input_tensor to the model.\n",
        "Interpret the prediction as either \"Positive\" or \"Negative\" based on the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "4LtJcRwdCfyK"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(model, input_tensor, model_name=\"Model\"):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    ############################# TODO #############################\n",
        "    # TODO: Perform a forward pass with the model on the input_tensor,\n",
        "    # get the predicted class label, and map it to \"Positive\" or \"Negative\".\n",
        "    ################################################################\n",
        "\n",
        "    # Perform a forward pass with the model\n",
        "    with torch.no_grad():  # We don't need gradients during inference\n",
        "        output = model(input_tensor)  # Forward pass\n",
        "\n",
        "    # Get the predicted class by finding the index with the highest probability\n",
        "    _, predicted_class = torch.max(output, dim=1)  # Get index of max probability\n",
        "\n",
        "    # Map the predicted class (assuming 0 = Negative, 1 = Positive)\n",
        "    class_label = \"Positive\" if predicted_class.item() == 1 else \"Negative\"\n",
        "\n",
        "\n",
        "    print(f\"The predicted class for the review by {model_name} is: {class_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "fHP6-svUCp_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de7799f-0e9b-4eef-92b1-9541bf52ef07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted class for the review by RNN is: Positive\n",
            "The predicted class for the review by Custom RNN is: Negative\n",
            "The predicted class for the review by LSTM is: Negative\n",
            "The predicted class for the review by Custom LSTM is: Negative\n"
          ]
        }
      ],
      "source": [
        "# Make predictions using with \"predict_sentiment\" function for each of four models above\n",
        "\n",
        "models = [rnn_model_1, rnn_model_2, lstm, custom_lstm]\n",
        "model_names = [\"RNN\", \"Custom RNN\", \"LSTM\", \"Custom LSTM\"]\n",
        "\n",
        "# Iterate through the models and make predictions\n",
        "for model, model_name in zip(models, model_names):\n",
        "    predict_sentiment(model, input_tensor, model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOclaPx9EOrY"
      },
      "source": [
        "# Questions\n",
        "\n",
        "[1] - Based on your observations, what do you think caused the difference in performance between the RNN and LSTM models (on test set)? Analyze this difference using the results from the notebook, and discuss where a simple RNN might perform better.\n",
        "\n",
        "[2] - If we increase max_len in the preprocessing step to 300, what changes in models (rnn & lstm ) performance would you expect, and why? Please *explain* and discuss the impact this may have on the learning process and the final results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer1-\n",
        "\n",
        "Results :\n",
        "\n",
        "Test Accuracy for LSTMClassifier: 42.90%\n",
        "\n",
        "Test Accuracy for Custom LSTM: 44.06%\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "Evaluating RNN Model 1:\n",
        "Accuracy: 48.57%\n",
        "\n",
        "Evaluating RNN Model 2:\n",
        "Accuracy: 51.92%\n",
        "\n",
        "----------------------------------------\n",
        "Since the length of the sentences in the test data is short, that is likely why RNNs have shown better performance compared to LSTMs.\n",
        "\n",
        "**Where RNNs Might Perform Better:**\n",
        "\n",
        "On datasets with shorter sequences or simpler patterns, RNNs might perform as well or even better than LSTMs. Their simplicity makes them less prone to overfitting and faster to train."
      ],
      "metadata": {
        "id": "W3QR62jlZYU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer2-\n",
        "\n",
        "Impact of Increasing max_len to 300\n",
        "Expected Changes in Performance:\n",
        "\n",
        "Improved Performance for LSTM:\n",
        "\n",
        "With longer sequences (max_len = 300), LSTMs would have more context to make predictions, which is particularly beneficial if the sentiment of a review depends on long-range dependencies. The gating mechanisms in LSTMs allow them to handle this increased sequence length without significant degradation in learning.\n",
        "\n",
        "Worsened or Unchanged Performance for RNN:\n",
        "\n",
        "Increasing the sequence length exacerbates the vanishing gradient problem in RNNs, making it even harder for them to learn from earlier tokens in the sequence. As a result, performance may deteriorate or plateau.\n",
        "\n",
        "Impact on the Learning Process:\n",
        "\n",
        "Training Time:\n",
        "\n",
        "Both RNNs and LSTMs would experience an increase in training time because of the additional computations required for longer sequences.\n",
        "LSTMs may require more epochs to fully utilize the additional context, as longer sequences make learning more complex.\n",
        "\n",
        "Risk of Overfitting:\n",
        "\n",
        "Both models, especially LSTMs, might be at a higher risk of overfitting due to the increased input size. Regularization techniques (dropout, weight decay) would become more critical.\n",
        "\n",
        "Memory Usage:\n",
        "\n",
        "Both models would require more memory to process longer sequences. LSTMs, being more complex, might push system limits in resource-constrained environments.\n",
        "\n",
        "Final Results:\n",
        "\n",
        "LSTM: Likely to achieve better results on the test set as long as the data contains relevant long-range dependencies and sufficient training data exists to learn them.\n",
        "\n",
        "RNN: Performance is unlikely to improve; in fact, it might degrade due to the challenges in learning long-term dependencies effectively."
      ],
      "metadata": {
        "id": "XD_wqe58ccye"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}